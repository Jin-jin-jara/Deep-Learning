{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEz9ngy4BvSLMwPM8S589e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jin-jin-jara/Deep-Learning/blob/master/keras_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrdxpGXf7EU_",
        "colab_type": "code",
        "outputId": "aef187e5-ba6c-4c11-c9e5-d30d573e1e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CYkttNb7FJY",
        "colab_type": "code",
        "outputId": "12d68204-1b5a-4a0f-8158-f3fa6679e076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x, y = load_iris(return_X_y=True)\n",
        "x.shape, y.shape, set(y)   # 150개의 데이터, 클래스 k = 3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 4), (150,), {0, 1, 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-IMQrTl7bPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.optimizer = tf.optimizers.SGD(learning_rate=0.01)   # stochastic gradient decent 확률적(배치로 나눠서 하기 때문에 stochastic이란 이름이 붙었다) 경사 하강\n",
        "    self.dense = keras.layers.Dense(units=3, activation=keras.activations.softmax)  # 클래스가 3개니까 unit은 3이다.\n",
        "\n",
        "  def call(self, x):\n",
        "    h = self.dense(x)\n",
        "    # h = h[:,0] # y가 1차원이기 떄문에 차원 축소를 해준다 2d -> 1d 멀티 클래스라 원핫인코딩을 해줘야해서 y는 2차원이 된다 하지 말자\n",
        "    return h\n",
        "\n",
        "  def get_loss(self, y, h):\n",
        "    # 분류이기 때문에 crossentropy 공식을 쓴다\n",
        "    cross_entropy = -(y * tf.math.log(h) + (1-y) * tf.math.log(1-h))\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss\n",
        "\n",
        "  def get_accuracy(self, y, h):\n",
        "    # h 3개의 확률 : (m, 3), y : (m)\n",
        "    predict = tf.argmax(h, -1)\n",
        "    # tf.cast 는 bool 형식의 값들을 0과 1로 만든다.\n",
        "    self.acc = tf.reduce_mean(tf.cast(tf.equal(y, predict), tf.float32))   # bool 값이 뜬다  True -> 1, False -> 0\n",
        "\n",
        "  def train(self, x, y, epoch=1):\n",
        "    # x : (m, 4) 2d, y : (m) 1d\n",
        "    y_hot = tf.one_hot(y, depth=3,)  # 2d로 변경  (m, 3)\n",
        "    # y_hot = tf.reshape(-1, 3)\n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape: # 경사 기록 장치\n",
        "        h = self.call(x)\n",
        "        loss = self.get_loss(y_hot, h)\n",
        "    grads = tape.gradient(loss, self.trainable_variables)  # 경사 계산\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_variables))   # 가중치에서 경사를 빼서 저장\n",
        "    self.get_accuracy(y, h)\n",
        "    print(\"%d/%d loss:%.3f acc:%.3f\"%(i, epoch, loss, self.acc))\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFE7BAmJtLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.opt = tf.optimizers.Adam(learning_rate=0.005)   # stochastic gradient decent 확률적(배치로 나눠서 하기 때문에 stochastic이란 이름이 붙었다) 경사 하강\n",
        "    self.conv1 = keras.layers.Conv2D(128,3, padding = 'same', input_shape=(8,8,1), activation=tf.nn.elu)\n",
        "    self.pooling1 = keras.layers.MaxPool2D( padding = 'same')\n",
        "    self.conv2 = keras.layers.Conv2D(64,3, padding='same', activation=tf.nn.elu)\n",
        "    self.pooling2 = keras.layers.MaxPool2D( padding = 'same')\n",
        "    self.flatten = keras.layers.Flatten()\n",
        "    self.dense1 = keras.layers.Dense(10, activation=keras.activations.softmax)\n",
        "    # self.dense2 = keras.layers.Dense(10, activation=keras.activations.relu)\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.reshape(x, [-1, 8, 8, 1])\n",
        "    x = self.conv1(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.flatten(x)\n",
        "    # x = self.dense2(x)\n",
        "    h = self.dense1(x)\n",
        "    return h\n",
        "    # h = h[:,0] # y가 1차원이기 떄문에 차원 축소를 해준다 2d -> 1d 멀티 클래스라 원핫인코딩을 해줘야해서 y는 2차원이 된다 하지 말자\n",
        "    \n",
        "\n",
        "  def get_loss(self, y, h):\n",
        "    # 분류이기 때문에 crossentropy 공식을 쓴다\n",
        "    h = tf.clip_by_value(h, 1e-8, 1 - 1e-8)\n",
        "    cross_entropy = -(y * tf.math.log(h) + (1-y) * tf.math.log(1-h))\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss\n",
        "\n",
        "  def get_accuracy(self, y, h):\n",
        "    # h 10개의 확률 : (m, 10), y : (m)\n",
        "    predict = tf.argmax(h, -1)\n",
        "    # tf.cast 는 bool 형식의 값들을 0과 1로 만든다.\n",
        "    self.acc = tf.reduce_mean(tf.cast(tf.equal(y, predict), tf.float32))   # bool 값이 뜬다  True -> 1, False -> 0\n",
        "\n",
        "  def train(self, x, y, epoch=1):\n",
        "    # x : (m, 4) 2d, y : (m) 1d\n",
        "    y_hot = tf.one_hot(y, depth=10)  # 2d로 변경  (m, 3)\n",
        "    # y_hot = tf.reshape(-1, 3)\n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape: # 경사 기록 장치\n",
        "        h = self.call(x)\n",
        "        loss = self.get_loss(y_hot, h)\n",
        "      \n",
        "      grads = tape.gradient(loss, self.trainable_variables)  # 경사 계산\n",
        "      self.opt.apply_gradients(zip(grads, self.trainable_variables))   # 가중치에서 경사를 빼서 저장\n",
        "      self.get_accuracy(y, h)\n",
        "      print(\"%d/%d loss:%.3f acc:%.3f\"%(i, epoch, loss, self.acc))\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niw70gyK_S_W",
        "colab_type": "code",
        "outputId": "dd6284f3-782b-47c4-a7ce-b918bee9b048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "x, y = load_digits(return_X_y=True)\n",
        "\n",
        "# x = np.reshape(x, (1797, 8, 8, 1))\n",
        "x.shape, y.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 64), (1797,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQufX2YMLB3K",
        "colab_type": "code",
        "outputId": "9ce7e932-a361-4b6c-8ac6-c41d924c5cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 64), (1797,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWg8aL08_l9G",
        "colab_type": "code",
        "outputId": "04f8cfff-9852-42e9-b6bf-48d13d034dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.train(x, y, 100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer conv2d_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "0/100 loss:0.409 acc:0.126\n",
            "1/100 loss:1.054 acc:0.098\n",
            "2/100 loss:1.326 acc:0.101\n",
            "3/100 loss:1.464 acc:0.100\n",
            "4/100 loss:1.201 acc:0.107\n",
            "5/100 loss:0.945 acc:0.147\n",
            "6/100 loss:0.922 acc:0.157\n",
            "7/100 loss:0.748 acc:0.196\n",
            "8/100 loss:0.536 acc:0.354\n",
            "9/100 loss:0.363 acc:0.301\n",
            "10/100 loss:0.447 acc:0.205\n",
            "11/100 loss:0.440 acc:0.265\n",
            "12/100 loss:0.275 acc:0.408\n",
            "13/100 loss:0.241 acc:0.496\n",
            "14/100 loss:0.237 acc:0.491\n",
            "15/100 loss:0.203 acc:0.611\n",
            "16/100 loss:0.178 acc:0.687\n",
            "17/100 loss:0.164 acc:0.705\n",
            "18/100 loss:0.151 acc:0.747\n",
            "19/100 loss:0.137 acc:0.748\n",
            "20/100 loss:0.125 acc:0.750\n",
            "21/100 loss:0.106 acc:0.809\n",
            "22/100 loss:0.085 acc:0.879\n",
            "23/100 loss:0.074 acc:0.899\n",
            "24/100 loss:0.072 acc:0.888\n",
            "25/100 loss:0.072 acc:0.883\n",
            "26/100 loss:0.065 acc:0.895\n",
            "27/100 loss:0.055 acc:0.918\n",
            "28/100 loss:0.047 acc:0.930\n",
            "29/100 loss:0.043 acc:0.937\n",
            "30/100 loss:0.040 acc:0.944\n",
            "31/100 loss:0.038 acc:0.946\n",
            "32/100 loss:0.037 acc:0.944\n",
            "33/100 loss:0.036 acc:0.943\n",
            "34/100 loss:0.033 acc:0.942\n",
            "35/100 loss:0.029 acc:0.950\n",
            "36/100 loss:0.026 acc:0.961\n",
            "37/100 loss:0.024 acc:0.968\n",
            "38/100 loss:0.023 acc:0.970\n",
            "39/100 loss:0.022 acc:0.970\n",
            "40/100 loss:0.020 acc:0.972\n",
            "41/100 loss:0.019 acc:0.974\n",
            "42/100 loss:0.018 acc:0.973\n",
            "43/100 loss:0.017 acc:0.974\n",
            "44/100 loss:0.017 acc:0.977\n",
            "45/100 loss:0.016 acc:0.979\n",
            "46/100 loss:0.015 acc:0.981\n",
            "47/100 loss:0.014 acc:0.982\n",
            "48/100 loss:0.014 acc:0.982\n",
            "49/100 loss:0.013 acc:0.983\n",
            "50/100 loss:0.012 acc:0.983\n",
            "51/100 loss:0.012 acc:0.985\n",
            "52/100 loss:0.011 acc:0.986\n",
            "53/100 loss:0.011 acc:0.987\n",
            "54/100 loss:0.010 acc:0.987\n",
            "55/100 loss:0.010 acc:0.988\n",
            "56/100 loss:0.010 acc:0.989\n",
            "57/100 loss:0.009 acc:0.991\n",
            "58/100 loss:0.009 acc:0.991\n",
            "59/100 loss:0.009 acc:0.991\n",
            "60/100 loss:0.008 acc:0.991\n",
            "61/100 loss:0.008 acc:0.992\n",
            "62/100 loss:0.008 acc:0.993\n",
            "63/100 loss:0.007 acc:0.994\n",
            "64/100 loss:0.007 acc:0.994\n",
            "65/100 loss:0.007 acc:0.995\n",
            "66/100 loss:0.007 acc:0.996\n",
            "67/100 loss:0.006 acc:0.996\n",
            "68/100 loss:0.006 acc:0.996\n",
            "69/100 loss:0.006 acc:0.997\n",
            "70/100 loss:0.006 acc:0.997\n",
            "71/100 loss:0.006 acc:0.998\n",
            "72/100 loss:0.005 acc:0.998\n",
            "73/100 loss:0.005 acc:0.998\n",
            "74/100 loss:0.005 acc:0.998\n",
            "75/100 loss:0.005 acc:0.998\n",
            "76/100 loss:0.005 acc:0.998\n",
            "77/100 loss:0.005 acc:0.998\n",
            "78/100 loss:0.005 acc:0.998\n",
            "79/100 loss:0.004 acc:0.998\n",
            "80/100 loss:0.004 acc:0.998\n",
            "81/100 loss:0.004 acc:0.998\n",
            "82/100 loss:0.004 acc:0.998\n",
            "83/100 loss:0.004 acc:0.998\n",
            "84/100 loss:0.004 acc:0.998\n",
            "85/100 loss:0.004 acc:0.998\n",
            "86/100 loss:0.004 acc:0.999\n",
            "87/100 loss:0.004 acc:0.999\n",
            "88/100 loss:0.003 acc:0.999\n",
            "89/100 loss:0.003 acc:0.999\n",
            "90/100 loss:0.003 acc:0.999\n",
            "91/100 loss:0.003 acc:0.999\n",
            "92/100 loss:0.003 acc:0.999\n",
            "93/100 loss:0.003 acc:0.999\n",
            "94/100 loss:0.003 acc:0.999\n",
            "95/100 loss:0.003 acc:0.999\n",
            "96/100 loss:0.003 acc:0.999\n",
            "97/100 loss:0.003 acc:0.999\n",
            "98/100 loss:0.003 acc:0.999\n",
            "99/100 loss:0.003 acc:0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt47X5EYB1z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 50/100 loss:0.021 acc:0.974\n",
        "# 50/100 loss:0.012 acc:0.986"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}