{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_2_number_with_blank(LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jin-jin-jara/Deep-Learning/blob/master/rnn_2_number_with_blank(LSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClP9Ucwu2BPH",
        "colab_type": "code",
        "outputId": "ba47aeb6-e705-4c20-a7cc-a1445333e729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpD1SwyB4aXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.set_floatx('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5nLNSOT2Iol",
        "colab_type": "code",
        "outputId": "c764433c-a082-4a8a-cdaa-ec3f812e6314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x, y = load_digits(return_X_y=True)\n",
        "x.shape, y.shape, set(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 64), (1797,), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucqLIsAHfGEy",
        "colab_type": "text"
      },
      "source": [
        "학습데이터와 테스트 데이터를 나눕니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmiRMc44dpfa",
        "colab_type": "code",
        "outputId": "033051df-536a-4fcf-face-2e49c6f7179f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m = len(y)//2\n",
        "x_train = x[:m]\n",
        "y_train = y[:m]\n",
        "x_test = x[m:m*2]\n",
        "y_test = y[m:m*2]\n",
        "x_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((898, 64), (898,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg9uIRTXQeFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_2_num(x_test, y_test):\n",
        "  x_test = np.reshape(x_test, [-1, 8, 8])\n",
        "  x_test_l, x_test_r = np.split(x_test, 2, axis=0) \n",
        "  x_test_lr = np.concatenate((x_test_l, x_test_r), axis=2)\n",
        "  y_test_l, y_test_r = np.split(y_test, 2, axis=0) \n",
        "  y_test = np.stack((y_test_l, y_test_r), -1)\n",
        "\n",
        "  #우측에 비어있는 이미지를 붙여서 한자리수 데이터를 만듭니다\n",
        "  BLANK = 10 #'없음' 을 나타내는 기호\n",
        "  x_test_blank = np.concatenate((x_test_l, np.zeros_like(x_test_r)), axis=2)\n",
        "  y_test_blank = np.stack((y_test_l, np.zeros_like(y_test_l) + BLANK), 1)#좌측 영상의 y와 빈 영상의 y를 통합\n",
        "\n",
        "  #두자리수 데이터와 한자리수 데이터를 통합합니다\n",
        "  x_test_set = np.concatenate((x_test_lr, x_test_blank), 0)# 두자리수 영상과 한자리수 영상을 통합\n",
        "  y_test_set = np.concatenate((y_test, y_test_blank), 0)#두자리수 y와 한자리수 y 를 통합\n",
        "  return x_test_set,  y_test_set  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7tkRlIj2HVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_set,  y_test_set = convert_2_num(x_test, y_test)\n",
        "x_train_set,  y_train_set = convert_2_num(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFknXaGK2o9C",
        "colab_type": "code",
        "outputId": "af1b9d8a-bdd8-4ad9-c6be-07d6edd18b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "i = 11\n",
        "plt.title('y: '+str(y_test_set[i]))\n",
        "plt.imshow(x_test_set[i].reshape((8,-1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed3bcec748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAADWCAYAAAD4p8hZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRQ0AmaOoASBzFDUAZI6iBoDM/T8zdTWcB0e6qQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUKdrSy746j6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n",
        "  def __init__(self):    \n",
        "    super(MyModel, self).__init__()\n",
        "    self.k = 10+1 # 클래스 갯수 \n",
        "    self.seq = 2 # 자릿수\n",
        "    self.opt = tf.optimizers.Nadam(learning_rate=0.008)#Stochatic Gradient Descent 확률적 경사 하강\n",
        "    self.conv0 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n",
        "    self.conv1 = keras.layers.Conv2D(64, [3,3], padding='same', activation=keras.activations.relu)\n",
        "    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n",
        "    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')    \n",
        "    self.rnn = keras.layers.LSTM(units=self.k, return_sequences=True)\n",
        "  \n",
        "  def call(self, x):\n",
        "    #x (1797, 64)\n",
        "    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n",
        "    x_4d = tf.cast(x_4d, tf.float32)\n",
        "    net = self.conv0(x_4d)\n",
        "    net = self.pool0(net)#(4,8,16)\n",
        "    net = self.conv1(net)\n",
        "    net = self.pool1(net)#(2,4,32)\n",
        "    net = self.pool1(net)#(1,2,32)\n",
        "    net = tf.squeeze(net, axis=1)#(2,32)\n",
        "    h = self.rnn(net)#(2,10)        \n",
        "    h = tf.nn.softmax(h, axis=2)\n",
        "    return h\n",
        "\n",
        "  def get_loss(self, y, h):\n",
        "    #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n",
        "    h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n",
        "    cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss\n",
        "\n",
        "  def get_accuracy(self, y, h):    \n",
        "    predict = tf.argmax(h, -1)\n",
        "    is_equal = tf.equal(y, predict)\n",
        "    self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n",
        "    self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n",
        "\n",
        "  def fit(self, x, y, epoch=1):\n",
        "    # x : (m, 8, 16), y: (m, 2)    \n",
        "    y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape: #경사 기록 장치\n",
        "        h = self.call(x)\n",
        "        loss = self.get_loss(y_hot, h)        \n",
        "      grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n",
        "      #경사가 너무 크면 nan 이 될 수 있으므로 gradient cliping (최소,최대값 제한) 을 합니다\n",
        "      grads = [(tf.clip_by_value(grad, -5.0, 5.0)) for grad in grads]      \n",
        "      self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n",
        "      self.get_accuracy(y, h)\n",
        "      if i%10==0:\n",
        "        print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hExFpO6F2hmG",
        "colab_type": "code",
        "outputId": "2f08721a-f51a-40ce-92aa-0bda7b1faa17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train_set, y_train_set, epoch=6000) #학습 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/6000 loss:0.308 acc:0.225 acc_all:0.000\n",
            "10/6000 loss:0.268 acc:0.326 acc_all:0.059\n",
            "20/6000 loss:0.256 acc:0.391 acc_all:0.120\n",
            "30/6000 loss:0.240 acc:0.398 acc_all:0.129\n",
            "40/6000 loss:0.230 acc:0.402 acc_all:0.133\n",
            "50/6000 loss:0.227 acc:0.403 acc_all:0.134\n",
            "60/6000 loss:0.226 acc:0.403 acc_all:0.134\n",
            "70/6000 loss:0.225 acc:0.403 acc_all:0.134\n",
            "80/6000 loss:0.225 acc:0.403 acc_all:0.134\n",
            "90/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "100/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "110/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "120/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "130/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "140/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "150/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "160/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "170/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "180/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "190/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "200/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "210/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "220/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "230/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "240/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "250/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "260/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "270/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "280/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "290/6000 loss:0.224 acc:0.403 acc_all:0.134\n",
            "300/6000 loss:0.243 acc:0.397 acc_all:0.116\n",
            "310/6000 loss:0.225 acc:0.345 acc_all:0.075\n",
            "320/6000 loss:0.224 acc:0.443 acc_all:0.169\n",
            "330/6000 loss:0.224 acc:0.427 acc_all:0.158\n",
            "340/6000 loss:0.224 acc:0.436 acc_all:0.170\n",
            "350/6000 loss:0.219 acc:0.462 acc_all:0.194\n",
            "360/6000 loss:0.219 acc:0.456 acc_all:0.190\n",
            "370/6000 loss:0.219 acc:0.460 acc_all:0.194\n",
            "380/6000 loss:0.219 acc:0.464 acc_all:0.197\n",
            "390/6000 loss:0.218 acc:0.464 acc_all:0.198\n",
            "400/6000 loss:0.218 acc:0.493 acc_all:0.222\n",
            "410/6000 loss:0.221 acc:0.480 acc_all:0.218\n",
            "420/6000 loss:0.219 acc:0.480 acc_all:0.225\n",
            "430/6000 loss:0.218 acc:0.481 acc_all:0.224\n",
            "440/6000 loss:0.218 acc:0.484 acc_all:0.225\n",
            "450/6000 loss:0.218 acc:0.485 acc_all:0.224\n",
            "460/6000 loss:0.218 acc:0.488 acc_all:0.225\n",
            "470/6000 loss:0.218 acc:0.492 acc_all:0.227\n",
            "480/6000 loss:0.218 acc:0.494 acc_all:0.231\n",
            "490/6000 loss:0.218 acc:0.496 acc_all:0.233\n",
            "500/6000 loss:0.218 acc:0.496 acc_all:0.233\n",
            "510/6000 loss:0.218 acc:0.496 acc_all:0.233\n",
            "520/6000 loss:0.218 acc:0.497 acc_all:0.233\n",
            "530/6000 loss:0.218 acc:0.498 acc_all:0.235\n",
            "540/6000 loss:0.218 acc:0.497 acc_all:0.235\n",
            "550/6000 loss:0.218 acc:0.497 acc_all:0.235\n",
            "560/6000 loss:0.218 acc:0.498 acc_all:0.235\n",
            "570/6000 loss:0.218 acc:0.498 acc_all:0.236\n",
            "580/6000 loss:0.218 acc:0.499 acc_all:0.236\n",
            "590/6000 loss:0.218 acc:0.500 acc_all:0.237\n",
            "600/6000 loss:0.218 acc:0.500 acc_all:0.237\n",
            "610/6000 loss:0.218 acc:0.501 acc_all:0.238\n",
            "620/6000 loss:0.218 acc:0.501 acc_all:0.238\n",
            "630/6000 loss:0.218 acc:0.501 acc_all:0.238\n",
            "640/6000 loss:0.218 acc:0.502 acc_all:0.241\n",
            "650/6000 loss:0.218 acc:0.502 acc_all:0.241\n",
            "660/6000 loss:0.218 acc:0.502 acc_all:0.241\n",
            "670/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "680/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "690/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "700/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "710/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "720/6000 loss:0.218 acc:0.503 acc_all:0.241\n",
            "730/6000 loss:0.218 acc:0.504 acc_all:0.241\n",
            "740/6000 loss:0.218 acc:0.504 acc_all:0.241\n",
            "750/6000 loss:0.218 acc:0.504 acc_all:0.241\n",
            "760/6000 loss:0.218 acc:0.504 acc_all:0.241\n",
            "770/6000 loss:0.218 acc:0.504 acc_all:0.242\n",
            "780/6000 loss:0.218 acc:0.505 acc_all:0.243\n",
            "790/6000 loss:0.218 acc:0.506 acc_all:0.243\n",
            "800/6000 loss:0.218 acc:0.506 acc_all:0.243\n",
            "810/6000 loss:0.218 acc:0.506 acc_all:0.243\n",
            "820/6000 loss:0.218 acc:0.506 acc_all:0.243\n",
            "830/6000 loss:0.218 acc:0.506 acc_all:0.243\n",
            "840/6000 loss:0.218 acc:0.507 acc_all:0.243\n",
            "850/6000 loss:0.218 acc:0.507 acc_all:0.244\n",
            "860/6000 loss:0.218 acc:0.507 acc_all:0.244\n",
            "870/6000 loss:0.218 acc:0.507 acc_all:0.244\n",
            "880/6000 loss:0.218 acc:0.507 acc_all:0.244\n",
            "890/6000 loss:0.218 acc:0.507 acc_all:0.244\n",
            "900/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "910/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "920/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "930/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "940/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "950/6000 loss:0.218 acc:0.508 acc_all:0.244\n",
            "960/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "970/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "980/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "990/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1000/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1010/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1020/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1030/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1040/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1050/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1060/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1070/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1080/6000 loss:0.218 acc:0.509 acc_all:0.244\n",
            "1090/6000 loss:0.218 acc:0.510 acc_all:0.245\n",
            "1100/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1110/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1120/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1130/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1140/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1150/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1160/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1170/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1180/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1190/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1200/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1210/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1220/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1230/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1240/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1250/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1260/6000 loss:0.218 acc:0.511 acc_all:0.246\n",
            "1270/6000 loss:0.218 acc:0.512 acc_all:0.247\n",
            "1280/6000 loss:0.218 acc:0.512 acc_all:0.247\n",
            "1290/6000 loss:0.218 acc:0.512 acc_all:0.247\n",
            "1300/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1310/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1320/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1330/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1340/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1350/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1360/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1370/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1380/6000 loss:0.218 acc:0.512 acc_all:0.248\n",
            "1390/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1400/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1410/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1420/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1430/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1440/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1450/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1460/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1470/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1480/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1490/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1500/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1510/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1520/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1530/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1540/6000 loss:0.218 acc:0.513 acc_all:0.249\n",
            "1550/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1560/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1570/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1580/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1590/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1600/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1610/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1620/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1630/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1640/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1650/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1660/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1670/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1680/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1690/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1700/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1710/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1720/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1730/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1740/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1750/6000 loss:0.218 acc:0.513 acc_all:0.251\n",
            "1760/6000 loss:0.218 acc:0.513 acc_all:0.251\n",
            "1770/6000 loss:0.218 acc:0.513 acc_all:0.251\n",
            "1780/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1790/6000 loss:0.218 acc:0.514 acc_all:0.251\n",
            "1800/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1810/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1820/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1830/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1840/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1850/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1860/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1870/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1880/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1890/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1900/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1910/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1920/6000 loss:0.218 acc:0.515 acc_all:0.251\n",
            "1930/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1940/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1950/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1960/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1970/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1980/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "1990/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "2000/6000 loss:0.218 acc:0.516 acc_all:0.251\n",
            "2010/6000 loss:0.218 acc:0.516 acc_all:0.252\n",
            "2020/6000 loss:0.218 acc:0.516 acc_all:0.252\n",
            "2030/6000 loss:0.218 acc:0.516 acc_all:0.252\n",
            "2040/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2050/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2060/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2070/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2080/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2090/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2100/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2110/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2120/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2130/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2140/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2150/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2160/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2170/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2180/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2190/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2200/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2210/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2220/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2230/6000 loss:0.218 acc:0.517 acc_all:0.253\n",
            "2240/6000 loss:0.218 acc:0.517 acc_all:0.254\n",
            "2250/6000 loss:0.218 acc:0.517 acc_all:0.254\n",
            "2260/6000 loss:0.218 acc:0.517 acc_all:0.254\n",
            "2270/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2280/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2290/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2300/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2310/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2320/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2330/6000 loss:0.218 acc:0.518 acc_all:0.254\n",
            "2340/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2350/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2360/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2370/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2380/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2390/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2400/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2410/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2420/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2430/6000 loss:0.218 acc:0.518 acc_all:0.255\n",
            "2440/6000 loss:0.218 acc:0.519 acc_all:0.256\n",
            "2450/6000 loss:0.218 acc:0.519 acc_all:0.257\n",
            "2460/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2470/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2480/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2490/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2500/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2510/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2520/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2530/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2540/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2550/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2560/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2570/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2580/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2590/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2600/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2610/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2620/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2630/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2640/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2650/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2660/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2670/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2680/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2690/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2700/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2710/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2720/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2730/6000 loss:0.218 acc:0.520 acc_all:0.257\n",
            "2740/6000 loss:0.218 acc:0.521 acc_all:0.257\n",
            "2750/6000 loss:0.218 acc:0.521 acc_all:0.257\n",
            "2760/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2770/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2780/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2790/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2800/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2810/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2820/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2830/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2840/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2850/6000 loss:0.218 acc:0.521 acc_all:0.258\n",
            "2860/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2870/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2880/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2890/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2900/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2910/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2920/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2930/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2940/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2950/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2960/6000 loss:0.218 acc:0.522 acc_all:0.258\n",
            "2970/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "2980/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "2990/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3000/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3010/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3020/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3030/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3040/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3050/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3060/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3070/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3080/6000 loss:0.218 acc:0.523 acc_all:0.258\n",
            "3090/6000 loss:0.218 acc:0.523 acc_all:0.258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_FA2_i0GLLh",
        "colab_type": "code",
        "outputId": "0ee510ef-fb57-48c2-8e1f-eb67d8abeaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# 테스트셋의 성능\n",
        "h = model(x_test_set)\n",
        "model.get_accuracy(y_test_set, h)\n",
        "print('개별정확도',model.acc.numpy(),'두자리 모두 맞춘 정확도', model.acc_all.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer my_model_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "개별정확도 0.69432074 두자리 모두 맞춘 정확도 0.4766147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFdhla3f4f0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n",
        "  def __init__(self):    \n",
        "    super(MyModel, self).__init__()\n",
        "    self.k = 10+1 # 클래스 갯수 \n",
        "    self.seq = 2 # 자릿수\n",
        "    self.opt = tf.optimizers.Nadam(learning_rate=0.001)#Stochatic Gradient Descent 확률적 경사 하강\n",
        "    self.conv0 = keras.layers.Conv2D(16, [3,3], padding='same', activation=keras.activations.relu)\n",
        "    self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n",
        "    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n",
        "    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')    \n",
        "    self.rnn = keras.layers.LSTM(units=self.k, return_sequences=True)\n",
        "    # self.rnn = keras.layers.GRU(units=self.k, return_sequences=True) # 조경현&요슈야 벤지오 2015\n",
        "    # self.rnn = keras.layers.Bidirectional(keras.layers.LSTM(units=self.k, return_sequences=True))\n",
        "  \n",
        "  def call(self, x):\n",
        "    #x (1797, 64)\n",
        "    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n",
        "    x_4d = tf.cast(x_4d, tf.float32)\n",
        "    net = self.conv0(x_4d)\n",
        "    net = self.pool0(net)#(4,8,16)\n",
        "    net = self.conv1(net)\n",
        "    net = self.pool1(net)#(2,4,32)\n",
        "    net = self.pool1(net)#(1,2,32)\n",
        "    # net = tf.reduce_sum(net, axis=1)  # (4, 32)\n",
        "    net = tf.squeeze(net, axis=1)#(2,32)  2: 2개의 시퀀스 \n",
        "    h = self.rnn(net)#(2,11)  # return_sequence=True 를 했기 때문에 h값에 아웃풋값 2개가 들어간다      \n",
        "    # h = self.rnn(net) # (4,11) 4개의 아웃풋\n",
        "    # h = h[:, 2:]  # 앞에 2개의 output은 버리고 뒤의 2개의 output만 사용\n",
        "    h = tf.nn.softmax(h, axis=2) \n",
        "    return h\n",
        "\n",
        "  def get_loss(self, y, h):\n",
        "    #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n",
        "    h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n",
        "    cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss\n",
        "\n",
        "  def get_accuracy(self, y, h):    \n",
        "    predict = tf.argmax(h, -1)\n",
        "    is_equal = tf.equal(y, predict)\n",
        "    self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n",
        "    self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n",
        "\n",
        "  def fit(self, x, y, epoch=1):\n",
        "    # x : (m, 8, 16), y: (m, 2)    \n",
        "    y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape: #경사 기록 장치\n",
        "        h = self.call(x)\n",
        "        loss = self.get_loss(y_hot, h)        \n",
        "      grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n",
        "      #경사가 너무 크면 nan 이 될 수 있으므로 gradient cliping (최소,최대값 제한) 을 합니다\n",
        "      grads = [(tf.clip_by_value(grad, -5.0, 5.0)) for grad in grads]      \n",
        "      self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n",
        "      self.get_accuracy(y, h)\n",
        "      if i%10==0:\n",
        "        print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}